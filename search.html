<!DOCTYPE html PUBLIC ""
    "">
<html><head><meta charset="UTF-8" /><title>Datalevin Search Engine</title><link rel="stylesheet" type="text/css" href="css/default.css" /><link rel="stylesheet" type="text/css" href="css/highlight.css" /><script type="text/javascript" src="js/highlight.min.js"></script><script type="text/javascript" src="js/jquery.min.js"></script><script type="text/javascript" src="js/page_effects.js"></script><script>hljs.initHighlightingOnLoad();</script></head><body><div id="header"><h2>Generated by <a href="https://github.com/weavejester/codox">Codox</a></h2><h1><a href="index.html"><span class="project-title"><span class="project-name">Datalevin</span> <span class="project-version">0.6.16</span></span></a></h1></div><div class="sidebar primary"><h3 class="no-link"><span class="inner">Project</span></h3><ul class="index-link"><li class="depth-1 "><a href="index.html"><div class="inner">Index</div></a></li></ul><h3 class="no-link"><span class="inner">Topics</span></h3><ul><li class="depth-1 "><a href="dtlv.html"><div class="inner"><span>Datalevin Command Line Tool</span></div></a></li><li class="depth-1  current"><a href="search.html"><div class="inner"><span>Datalevin Search Engine</span></div></a></li><li class="depth-1 "><a href="server.html"><div class="inner"><span>Datalevin Server/Client</span></div></a></li><li class="depth-1 "><a href="upgrade.html"><div class="inner"><span>Datalevin Database Upgrade</span></div></a></li></ul><h3 class="no-link"><span class="inner">Namespaces</span></h3><ul><li class="depth-1"><div class="no-link"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>datalevin</span></div></div></li><li class="depth-2 branch"><a href="datalevin.client.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>client</span></div></a></li><li class="depth-2 branch"><a href="datalevin.core.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>core</span></div></a></li><li class="depth-2 branch"><a href="datalevin.interpret.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>interpret</span></div></a></li><li class="depth-2"><a href="datalevin.search-utils.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>search-utils</span></div></a></li></ul></div><div class="document" id="content"><div class="doc"><div class="markdown"><h1><a href="#datalevin-search-engine" name="datalevin-search-engine"></a>Datalevin Search Engine</h1>
<p>Datalevin includes a built-in full-text search engine.</p>
<h2><a href="#rationale" name="rationale"></a>Rationale</h2>
<p>Traditionally, databases and search engines are separate technology fields. However, from the point of view of an end user, there is hardly a reason why these two should be separated. A database is for storing and querying data, so is a search engine. Although many databases have some full-text search capabilities, their performance in term of relevance and speed is limited compared with standalone search engines.</p>
<p>In a Datalog database, a full-text search function can be seen as just another function or predicate to be used in a query. For example, Datomic On-Prem has a <code>fulltext</code> function that allows full text search on a single attribute, which is implemented with <a href="https://lucene.apache.org/">Apache Lucene</a> search engine.</p>
<p>Datalevin’s built-in full-text search engine supports more powerful search across the whole database. The reason why developing a search engine of our own for Datalevin, instead of using an existing search engine, is the following:</p>
<p>Standalone search engines, such as Apache Lucene, often have to implement simple database-like concepts, such as <code>fields</code>, in order to capture structural information of documents. In fact, people often use them as specialized databases because of that, e.g. Elasticsearch. Datalevin is a versatile Database with full featured database transaction and query functionalities. Including a standalone search engine would introduce redundant and unnecessary database-like features that are less powerful and not as well integrated with the rest of the database.</p>
<p>Another way to look at the issue is through the sizes of the dependencies. Popular search engines, such as Lucene, have huge code base with many features. The library jar of lucene-core, not including any options or modules, is 3.4 MB (the whole package download is more than 85 MB), whereas the total size of Datalevin library jar is 120 KB.</p>
<p>Finally, with a search engine of our own, we avoid unnecessary write amplification introduced by storing the source text twice, once in the database, again in the search engine. Instead, the embedded search engine only needs to store a reference to the source content that is stored in the database.</p>
<h2><a href="#usage" name="usage"></a>Usage</h2>
<p>The full-text search functionalities are available to use in all supported Datalevin modes: key-value store, Datalog store, embedded, client/server, or Babashka pods.</p>
<h3><a href="#standalone-search" name="standalone-search"></a>Standalone search</h3>
<p>Datalevin can be used as a standalone search engine. The standalone search API involves only a few functions: <code>new-search-engine</code>, <code>add-doc</code>, <code>remove-doc</code>, and <code>search</code>.</p>
<pre><code class="Clojure">(require '[datalevin.core :as d])

;; A search engine depends on a key-value store to store the indices.
(def lmdb (d/open-kv "/tmp/search-db"))
(def engine (d/new-search-engine lmdb))

;; Here are the documents to be indxed, keyed by doc-id
(def docs
  {1 "The quick red fox jumped over the lazy red dogs."
   2 "Mary had a little lamb whose fleece was red as fire."
   3 "Moby Dick is a story of a whale and a man obsessed."})

;; Add the documents into the search index. `add-doc` takes a `doc-ref`, which
;; can be anything that uniquely identify a document, in this case, a doc-id
(d/add-doc engine 1 (docs 1))
(d/add-doc engine 2 (docs 2))
(d/add-doc engine 3 (docs 3))

;; Search engine does not store the raw documents themselves.
;; If we want to retrieve the found documents, we can optionally store them in
;; a key-value sub-database
(d/open-dbi lmdb "raw")
(d/transact-kv lmdb
      [[:put "raw" 1 (docs 1)]
       [:put "raw" 2 (docs 2)]
       [:put "raw" 3 (docs 3)]])

;; search by default return a list of `doc-ref` ordered by relevance to query
(d/search engine "red")
;=&gt; (1 2)

;; we can alter the display to show offets of term occurrences as well, useful
;; e.g. to highlight matched terms in documents
(d/search engine "red" {:display :offsets})
;=&gt; ([1 (["red" [10 39]])] [2 (["red" [40]])])

</code></pre>
<h3><a href="#search-in-datalog" name="search-in-datalog"></a>Search in Datalog</h3>
<p>Searchable values of the Datalog attributes need to be declared in the schema, with the <code>:db/fulltext true</code> property. The value does not have to be of  string type, as the indexer will call <code>str</code> function on it to convert it to  string.</p>
<p>A query function <code>fulltext</code> is provided to allow full-text search in Datalog queries. This function takes the db, the query and an optional option map (same as <code>search</code>), and returns a sequence of matching datoms, ordered by relevance to the query.</p>
<pre><code class="Clojure">(let [db (-&gt; (d/empty-db "/tmp/mydb" 
               {:text {:db/valueType :db.type/string
                       :db/fulltext  true}})
             (d/db-with
                 [{:db/id 1,
                   :text  "The quick red fox jumped over the lazy red dogs."}
                  {:db/id 2,
                   :text  "Mary had a little lamb whose fleece was red as fire."}
                  {:db/id 3,
                   :text  "Moby Dick is a story of a whale and a man obsessed."}]))]
    (d/q '[:find ?e ?a ?v
           :in $ ?q
           :where [(fulltext $ ?q) [[?e ?a ?v]]]]
          db
          "red fox"))
;=&gt; #{[1 :text "The quick red fox jumped over the lazy red dogs."]
;     [2 :text "Mary had a little lamb whose fleece was red as fire."]}
</code></pre>
<p>In the above example, we destructure the returned datoms into three variables, <code>?e</code>, <code>?a</code> and <code>?v</code>.</p>
<p>As can be seen, the search is across the whole database, not limited to an individual attribute.</p>
<p>To further filter the search results, a <code>doc-filter</code> function can be supplied in the search option, that takes the <code>doc-ref</code> and return true or false, e.g. <code>{:doc-filter #(= (:a %) :text)}</code> will only return datoms that have attribute <code>:text</code>. Or one can opt to put this constraint in the Datalog where clause instead.</p>
<h2><a href="#implementation" name="implementation"></a>Implementation</h2>
<p>As mentioned, the search engine is implemented from scratch, see <a href="https://yyhh.org/blog/2021/11/t-wand-beat-lucene-in-less-than-600-lines-of-code/">blog</a>. Instead of using a separate storage, the search engine indices are stored in the same file along with other database data, i.e. all Datalevin data is stored in a single <a href="http://www.lmdb.tech/doc/">LMDB</a> data file. This improves cache locality and reduces the complexity of managing data.</p>
<h3><a href="#indexing" name="indexing"></a>Indexing</h3>
<p>The search engine indices are stored in one inverted list and two key-value maps. In addition to information about each term and each document, the positions of term occurrences in the documents are also stored to support match highlighting, proximity query (planned), and phrase query (planned).</p>
<p>Specifically, the following LMDB sub-databases are created for search supposes:</p>
<ul>
  <li><code>terms</code>: map of term -&gt; <code>term-info</code></li>
  <li><code>docs</code>: map of document id -&gt; document reference and document norm</li>
  <li><code>positions</code>: inverted lists of term id and document id -&gt; positions and offsets  of the term in the document</li>
</ul>
<p>The inverted list implementation leverages the <code>DUPSORT</code> feature of LMDB, where multiple values (i.e. the list) of the same key are stored together in sorted order, allowing efficient iteration, count and update.</p>
<p>Most information needed for scoring documents were pre-calculated during indexing, and loaded into memory on demand. For example, the norms of all documents were loaded into memory during search engine initialization, the same as Lucene. Term specific information are loaded into memory in aggregates at the beginning of query processing.</p>
<p>Specifically, a <code>term-info</code> consists of the following information about a term:</p>
<ul>
  <li><code>term-id</code>, unique id of the term, assigned auto-incrementally during indexing</li>
  <li><code>max-weight</code>, the maximum weight of the term</li>
  <li><code>doc-freq-sparse-list</code>, consists of compressed bitmap of document ids and  corresponding list of term frequencies</li>
</ul>
<p><code>doc-freq-sparse-list</code> uses our implementation of a sparse integer list, constructed with two data structures working together. One structure is the index, containing document ids, represented by a bitmap; and the other is an array list of integers, containing term frequencies. This is the primary data structure for searching, and they are loaded into memory per user query terms. The list of a term is un/compressed during retrieval/storage as a whole.</p>
<h3><a href="#searching" name="searching"></a>Searching</h3>
<p>Scoring and ranking of documents implements the standard <code>tf-idf</code> and vector space model. In order to achieve a good balance between relevance and efficiency, the weighting scheme chosen is <code>lnu.ltn</code> [2], i.e. the document vector has log-weighted term frequency, no idf, and pivoted unique normalization, while the query vector uses log-weighted term frequency, idf weighting, and no normalization. One significant difference from the standard <code>BM25</code> schema (used in Lucne) is the document normalization method. Pivoted unique normalization is chosen for it takes document lengths into consideration, does not needlessly penalize lengthy documents, and it is cheaper to calculate.</p>
<p>An original algorithm, what we call <em>T-Wand</em>, is developed and implemented for searching. <em>T</em> stands for “Tiered”, <em>Wand</em> [1] is a state of art search algorithm used in many search engines, including Lucene. Our algorithm is an marriage of the main ideas of <em>Wand</em> and some of our owns.</p>
<p>As the name suggests, our algorithm works in tiers. It removes early on those documents that are unlikely to be relevant due to missing query terms. Not only is it efficient, this approach also addresses an often felt user frustration with search engines: a document containing all query terms may be ranked much lower than a document containing only partial query terms. In our algorithm, the documents containing more query terms are considered first and are guaranteed to return earlier than those documents with poorer term coverage.</p>
<h4><a href="#t-wand-algorithm" name="t-wand-algorithm"></a>T-Wand algorithm</h4>
<p>The details of the <em>T-Wand</em> algorithm is the following.</p>
<p>First, we want to consider documents containing all <code>n</code> user specified query terms. Instead of looping over all <code>n</code> inverted lists of all query terms, we first pick the query term with the least edit distance (i.e. with the least amount of typos) and the least document frequency (i.e. the most rare term), use its inverted list of documents as the candidates, and forgo all other documents. This is sufficient, because, for a document to contain all <code>n</code> query terms, it must contains the rarest one among them. More generally, there is a simple (trivial?) mathematical property that can be stated as the following [3]:</p>
<pre><code>Let there be a set X with size n and a set Y with any size. For any subset Z in
X of size (n-t+1), if the size of the intersection of X and Y is greater or equal
to t, then Z must intersect with Y.
</code></pre>
<p>This property allows us to develop search algorithms that use the inverted lists of any  <code>n-t+1</code> terms alone as the candidates, in order to find documents that contain <code>t</code>  query terms. We will of course choose the rarest ones (i.e. shortest lists) to  start with. For example, to search for documents with all <code>n</code> query terms, it  is sufficient to search in documents containing the rarest term; to search for  documents with <code>n-1</code> terms, it is sufficient to search in the union of the  rarest and the second rarest term’s documents; so on and so forth. When there  are very rare terms in the query, very few candidate documents need to be  examined.</p>
<p>We loop over this list of candidate documents, for each document, check if it appears in the inverted lists of subsequent query terms.</p>
<p>Critically, during this process, we remove a candidate as soon as one of the following two conditions is met:</p>
<ul>
  <li>this document is not going to appear in the required number of inverted lists,  based on the aforementioned mathematical property;</li>
</ul>
<p>or,</p>
<ul>
  <li>this document is not going to make into the top K results, based on an  approximate score calculation using the pre-computed maximum weight of the  terms. This is the main idea behind <em>Wand</em> algorithm’s search efficiency [1],  as it allows skipping full scoring of many documents.</li>
</ul>
<p>If all candidates are exhausted and user still requests more results, the documents containing the second rarest query term are added to candidates list. They are checked against the remaining query terms to select the candidates that appear in <code>n-1</code> inverted lists. If user keeps asking for more results, the process continues until candidates only need to appear in <code>1</code> inverted list.</p>
<p>Essentially, this search algorithm processes documents in tiers. First tier are those documents containing all <code>n</code> query terms, then <code>n-1</code> terms, then <code>n-2</code>, and so on. Document ranking is performed within a tier, not cross tiers. As the number of tiers goes up, the query is gradually turned from an all-AND to an all-OR query.</p>
<h4><a href="#t-wand-implementation" name="t-wand-implementation"></a><em>T-Wand</em> implementation</h4>
<p>The code is written in Clojure. The whole search engine weights less than 600 lines of code. We have not unduly optimized the Clojure code for performance. That is to say, we have left many search speed optimization opportunities on the table, by writing idiomatic Clojure for the most part.</p>
<p>As can be seen, the implementation of <em>T-Wand</em> relies heavily on intersection and union of document ids. Our implementation is helped by <a href="https://roaringbitmap.org/">Roaring Bitmaps</a>, a fast compressed bitmap library used in many projects. The parallel walking of document ids of different terms required by <em>Wand</em> is achieved by iterating the bitmaps.</p>
<p>As mentioned, our storage of term frequencies is handled by an integer array list, indexed by a bitmap of document ids. The access of a term frequency is through the <code>rank</code> method of Roaring Bitmaps, which seems to be an innovation, as far as I can tell. When stored, the integer list is compressed with <a href="https://github.com/lemire/JavaFastPFOR">JavaFastPFOR</a>, another excellent library by Prof. Daniel Lemire, the same author of Roaring Bitmaps.</p>
<p><a href="https://www.eclipse.org/collections/">Eclipse Collections</a> is used to reduce memory footprint whenever appropriate.</p>
<p>The query result preparation are implemented as Clojure transducers, and the results are wrapped in the <code>sequence</code> function, which returns results incrementally and on demand.</p>
<h2><a href="#benchmark" name="benchmark"></a>Benchmark</h2>
<p>The details of benchmark comparison with Lucene is <a href="https://github.com/juji-io/datalevin/tree/master/search-bench">here</a>. The summary is that Datalevin search engine beats Lucene in search speed, but lags in write speed, as expected.</p>
<h2><a href="#references" name="references"></a>References</h2>
<p>[1] Broder, A.Z., Carmel, D., Herscovici, M, Soffer,A., and Zien, J.. Efficient query evaluation using a two-level retrieval process. In Proceedings of the twelfth international conference on Information and knowledge management (CIKM ’03). 2003. pp.426–434.</p>
<p>[2] Manning, C.D., Raghavan, P. and Schütze, H. Introduction to Information Retrieval, Cambridge University Press. 2008.</p>
<p>[3] Okazaki, N. and Tsujii, J., Simple and Efficient Algorithm for Approximate Dictionary Matching. Proceedings of the 23rd International Conference on Computational Linguistics (COLING ’10), 2010, pp. 851-859.</p></div></div></div></body></html>